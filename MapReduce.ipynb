{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd9ba56-0e36-44f6-bf28-33034046dd5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a025563b-f39a-4959-8157-45c538f06865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package installation and imports - run this first\n",
    "\"\"\"\n",
    "import importlib.util, sys, subprocess\n",
    "packages = [ 'pandas', 'pyarrow', 'requests', 'tqdm', \n",
    "             'geopandas', 'shapely', 'IPython',\n",
    "            'fiona', 'pyproj', 'folium', 'meteostat',\n",
    "           'pathlib', 'datetime', 'collections']\n",
    "for package_name in packages:\n",
    "    is_present = importlib.util.find_spec(package_name)\n",
    "    if is_present is None:\n",
    "        print(f\"{package_name} is not installed\")\n",
    "        !pip install {package_name}\n",
    "        print(f\"{package_name} is now installed\")\n",
    "    else:\n",
    "        print(f\"{package_name} is installed\")\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f19718-40cd-4b6b-a01f-f7d5af496173",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b1e33f-8d1c-4248-bb31-0efce53eb2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create spark session\n",
    "\n",
    "try:\n",
    "    import pyspark\n",
    "except ImportError:\n",
    "    !pip install pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc9271-acb8-495a-b51e-ed7cb2a9f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER = \"local[*]\"   #\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"TLC-Notebook\")\n",
    "    .master(MASTER)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fbcd7e-bed1-4522-9c1d-e9d8f844fd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize paths\n",
    "\n",
    "IN_TLC   = \"data/tlc\"                 # parquet inputs (all TLC months)\n",
    "IN_WX    = \"data/weather/hourly\"      # your monthly weather parquet files\n",
    "OUT_ZONE = \"out/zone_hour_dropoff\"    # notebook writes here\n",
    "OUT_OD   = \"out/od_flows\"\n",
    "OUT_WXJ  = \"out/weather_join\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf05850-cf14-4fd8-81b1-3905c29978a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hand column-name drift\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def pick(cols, candidates):\n",
    "    \"\"\"Return the first matching column from candidates (case-insensitive).\"\"\"\n",
    "    low = {c.lower(): c for c in cols}\n",
    "    for c in candidates:\n",
    "        if c.lower() in low:\n",
    "            return low[c.lower()]\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d3479a-9049-4258-abd9-5e88d0e31b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zone × hour demand (pickup or dropoff)\n",
    "# Read all TLC parquet; compute trips per (zone, hour); write parquet\n",
    "df = spark.read.parquet(IN_TLC)\n",
    "\n",
    "PU = [\"PULocationID\",\"PUlocationID\",\"pu_location_id\"]\n",
    "DO = [\"DOLocationID\",\"DOlocationID\",\"do_location_id\"]\n",
    "TP = [\"tpep_pickup_datetime\",\"lpep_pickup_datetime\",\"pickup_datetime\",\"request_datetime\"]\n",
    "TD = [\"tpep_dropoff_datetime\",\"lpep_dropoff_datetime\",\"dropoff_datetime\",\"dropOff_datetime\"]\n",
    "\n",
    "KIND = \"dropoff\"  # change to \"pickup\" if you want pickup hotspots\n",
    "zone_col = pick(df.columns, DO if KIND == \"dropoff\" else PU)\n",
    "time_col = pick(df.columns, TD if KIND == \"dropoff\" else TP)\n",
    "assert zone_col and time_col, \"Required TLC columns not found\"\n",
    "\n",
    "result = (\n",
    "    df.filter(F.col(zone_col).isNotNull())                       # keep rows with a zone id\n",
    "      .withColumn(\"zone\", F.col(zone_col).cast(\"int\"))           # cast zone to int\n",
    "      .withColumn(\"ts\", F.to_timestamp(F.col(time_col)))         # parse timestamp\n",
    "      .withColumn(\"hour\", F.date_trunc(\"hour\", F.col(\"ts\")))     # truncate to hour\n",
    "      .groupBy(\"zone\",\"hour\").count()                            # aggregate counts\n",
    "      .withColumnRenamed(\"count\",\"trips\")                        # name output col\n",
    ")\n",
    "\n",
    "result.write.mode(\"overwrite\").parquet(OUT_ZONE)\n",
    "print(\"Wrote:\", OUT_ZONE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1ee221-6ab8-4048-9112-94ea9e209650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OD flows (PU → DO) by hour\n",
    "\n",
    "df = spark.read.parquet(IN_TLC)\n",
    "\n",
    "pu = pick(df.columns, PU)\n",
    "do = pick(df.columns, DO)\n",
    "tp = pick(df.columns, TP)\n",
    "assert pu and do and tp, \"Required TLC columns not found\"\n",
    "\n",
    "flows = (\n",
    "    df.filter(F.col(pu).isNotNull() & F.col(do).isNotNull())     # require both PU & DO\n",
    "      .withColumn(\"pu\", F.col(pu).cast(\"int\"))                   # cast ids to int\n",
    "      .withColumn(\"do\", F.col(do).cast(\"int\"))\n",
    "      .withColumn(\"hour\", F.date_trunc(\"hour\", F.to_timestamp(F.col(tp))))  # hour key\n",
    "      .groupBy(\"pu\",\"do\",\"hour\").count()                         # aggregate OD counts\n",
    "      .withColumnRenamed(\"count\",\"trips\")\n",
    ")\n",
    "\n",
    "flows.write.mode(\"overwrite\").parquet(OUT_OD)\n",
    "print(\"Wrote:\", OUT_OD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b444cecc-f578-4cdd-b79a-a7dfa951714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weather join → citywide hourly series\n",
    "\n",
    "# Load zone-hour demand and weather; join on 'hour'; save citywide hourly series\n",
    "dem = spark.read.parquet(OUT_ZONE)     # from step 3\n",
    "wx  = spark.read.parquet(IN_WX)        # your hourly weather parquet\n",
    "\n",
    "# Detect timestamp column in weather parquet (depends on how you saved it)\n",
    "ts_col = \"time\" if \"time\" in wx.columns else (\"index\" if \"index\" in wx.columns else None)\n",
    "assert ts_col, \"Weather parquet must have a 'time' (or 'index') column\"\n",
    "\n",
    "wx_hour = (\n",
    "    wx.withColumn(\"hour\", F.date_trunc(\"hour\", F.to_timestamp(F.col(ts_col))))  # normalize to hour\n",
    "      .select(\"hour\", *[c for c in [\"prcp\",\"temp\"] if c in wx.columns])         # keep prcp/temp if present\n",
    ")\n",
    "\n",
    "joined = dem.join(wx_hour, on=\"hour\", how=\"left\")                                # left join demand to weather\n",
    "\n",
    "city = (\n",
    "    joined.groupBy(\"hour\")                                                       # citywide hourly\n",
    "          .agg(F.sum(\"trips\").alias(\"trips\"),\n",
    "               F.first(\"prcp\").alias(\"prcp\"),\n",
    "               F.first(\"temp\").alias(\"temp\"))\n",
    ")\n",
    "\n",
    "city.write.mode(\"overwrite\").parquet(OUT_WXJ)\n",
    "print(\"Wrote:\", OUT_WXJ)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
