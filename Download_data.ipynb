{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "feb1a7d8-f89d-4586-8128-0dd42535ca7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas is installed\n",
      "pyarrow is installed\n",
      "requests is installed\n",
      "tqdm is installed\n",
      "scikit-learn is not installed\n",
      "scikit-learn is now installedRequirement already satisfied: scikit-learn in c:\\users\\kolobet01\\appdata\\local\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\kolobet01\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\kolobet01\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kolobet01\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kolobet01\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "\n",
      "matplotlib is installed\n",
      "seaborn is installed\n",
      "numpy is installed\n",
      "boto3 is installed\n",
      "botocore is installed\n",
      "geopandas is installed\n",
      "shapely is installed\n",
      "fiona is installed\n",
      "pyproj is installed\n",
      "rtree is installed\n",
      "folium is installed\n",
      "meteostat is installed\n"
     ]
    }
   ],
   "source": [
    "# Package installation and imports - run this first\n",
    "import importlib.util\n",
    "packages = [ 'pandas', 'pyarrow', 'requests', 'tqdm','scikit-learn', 'matplotlib', \n",
    "            'seaborn', 'numpy', 'boto3', 'botocore', 'geopandas', 'shapely', \n",
    "            'fiona', 'pyproj', 'rtree','folium', 'meteostat']\n",
    "for package_name in packages:\n",
    "    is_present = importlib.util.find_spec(package_name)\n",
    "    if is_present is None:\n",
    "        print(f\"{package_name} is not installed\")\n",
    "        !pip install {package_name}\n",
    "        print(f\"{package_name} is now installed\")\n",
    "    else:\n",
    "        print(f\"{package_name} is installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2953a731-1775-451b-a237-54bf46f27f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import time, os\n",
    "from typing import Optional, Tuple\n",
    "import requests\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    _HAS_TQDM = True\n",
    "except Exception:\n",
    "    _HAS_TQDM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a78d3366-f2a1-4b2a-87bf-c57f30007fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output root: C:\\Users\\kolobet01\\MIT805-Semester-Project-Assignment\\data\\tlc\n"
     ]
    }
   ],
   "source": [
    "years_st = [2023]\n",
    "url_base = 'https://d37ci6vzurychx.cloudfront.net/trip-data'\n",
    "datasts = ['fhvhv', 'yellow', 'fhv']   # taxi data sets to be downloaded\n",
    "months = list(range(1, 13))             # all months of the year\n",
    "file_formt = 'parquet'                 # TLC shares files as parquet\n",
    "out_put_rt = Path('data/tlc')          # folder to download data to\n",
    "\n",
    "start = datetime(min(YEARS), 1, 1)      # start time for weather download\n",
    "end   = datetime(max(YEARS), 12, 31, 23, 59) \n",
    "\n",
    "weather_folder = Path(\"data/weather/hourly\")  # Folder to download weather data to\n",
    "weather_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "skip_flag = True # flag to avoid multiple downloads of the same file\n",
    "check_remote_size = True  # if no Content-Length from remote server , use local file\n",
    "\n",
    "out_put_rt.mkdir(parents=True, exist_ok=True)\n",
    "print('Output root:', out_put_rt.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e727182-3ff0-43d7-bbc9-396be0d8c1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ddc3373-5712-49e4-bf03-24b32bc3b390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data URL, expected formats and data sets\n",
    "#CF_BASE = 'https://d37ci6vzurychx.cloudfront.net/trip-data'\n",
    "#VALID_DATASETS = {'yellow','fhv','fhvhv'}\n",
    "#VALID_FORMATS  = {'parquet'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfbaf421-8191-4b06-85ee-211c6b3bf606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define url that combines the data link, year, months and extention to get correct data fro TLC\n",
    "def build_url(dataset: str, year: int, month: int, ext: str) -> str:\n",
    "    ds = dataset.lower()\n",
    "    return f\"{CF_BASE}/{ds}_tripdata_{year}-{month:02d}.{ext}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60dbc31f-158f-4a8d-b889-f8def18bb6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return remote Content-Length to compare with local lenth if file has previously been downloaded\n",
    "def Remote_File_content_length(url: str, timeout: float = 30.0):\n",
    "\n",
    "    try:\n",
    "        r = requests.head(url, timeout=timeout, allow_redirects=True)\n",
    "        if r.status_code == 200:\n",
    "            cl = r.headers.get(\"Content-Length\")\n",
    "            return int(cl) if cl is not None else None\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89ba2e2c-d215-43ff-b8e8-e6f7c8683da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to download file from rmote server\n",
    "# download file, show progress, retry and save to folder\n",
    "def data_download(url: str, dest: Path, max_retries: int = 3, backoff: float = 2.0):\n",
    "    headers = {'User-Agent': 'nyc-tlc-downloader/size-only/1.0'}\n",
    "    for attempt in range(1, max_retries+1):\n",
    "        try:\n",
    "            with requests.get(url, stream=True, timeout=60, headers=headers) as r:\n",
    "                if r.status_code == 404:\n",
    "                    print('404 Not Found:', url)\n",
    "                    return None\n",
    "                r.raise_for_status()\n",
    "                total = int(r.headers.get('Content-Length', 0))\n",
    "                dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "                pbar = tqdm(total=total, unit='B', unit_scale=True, desc=dest.name) if (_HAS_TQDM and total) else None\n",
    "                with dest.open('wb') as f:\n",
    "                    for chunk in r.iter_content(chunk_size=1024*1024):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                            if pbar: pbar.update(len(chunk))\n",
    "                if pbar: pbar.close()\n",
    "                return dest.stat().st_size\n",
    "        except Exception as e:\n",
    "            wait = backoff ** attempt\n",
    "            print(f'Error downloading {url} (attempt {attempt}/{max_retries}): {e}. Retrying in {wait:.1f}s...')\n",
    "            time.sleep(wait)\n",
    "    print(f'Failed to download after {max_retries} attempts:', url)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5170b496-e4d3-48fe-8a06-e962b18dcc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search through local file before downloading and return if found\n",
    "\n",
    "def find_local_file(out_dir: Path, dataset: str, year: int, month: int, prefer_ext: str):\n",
    "\n",
    "    cand1 = out_dir / f\"{dataset}_tripdata_{year}-{month:02d}.{prefer_ext}\"\n",
    "    other = 'csv' if prefer_ext == 'parquet' else 'parquet'\n",
    "    cand2 = out_dir / f\"{dataset}_tripdata_{year}-{month:02d}.{other}\"\n",
    "    if cand1.exists(): return (cand1, prefer_ext)\n",
    "    if cand2.exists(): return (cand2, other)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2128b023-5c9f-451e-a848-bf807cab2505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] fhvhv_tripdata_2023-01.parquet (exists; size check matched)\n",
      "[SKIP] fhvhv_tripdata_2023-02.parquet (exists; size check matched)\n",
      "[SKIP] fhvhv_tripdata_2023-03.parquet (exists; size check matched)\n",
      "[SKIP] fhvhv_tripdata_2023-04.parquet (exists; size check matched)\n",
      "[SKIP] fhvhv_tripdata_2023-05.parquet (exists; size check matched)\n",
      "[SKIP] fhvhv_tripdata_2023-06.parquet (exists; size check not available -> trusted)\n",
      "[SKIP] fhvhv_tripdata_2023-07.parquet (exists; size check not available -> trusted)\n",
      "[SKIP] fhvhv_tripdata_2023-08.parquet (exists; size check not available -> trusted)\n",
      "[SKIP] fhvhv_tripdata_2023-09.parquet (exists; size check not available -> trusted)\n",
      "[SKIP] fhvhv_tripdata_2023-10.parquet (exists; size check not available -> trusted)\n",
      "[SKIP] fhvhv_tripdata_2023-11.parquet (exists; size check matched)\n",
      "[SKIP] fhvhv_tripdata_2023-12.parquet (exists; size check matched)\n",
      "[SKIP] yellow_tripdata_2023-01.parquet (exists; size check matched)\n",
      "[SKIP] yellow_tripdata_2023-02.parquet (exists; size check matched)\n",
      "[SKIP] yellow_tripdata_2023-03.parquet (exists; size check matched)\n",
      "[SKIP] yellow_tripdata_2023-04.parquet (exists; size check matched)\n",
      "[SKIP] yellow_tripdata_2023-05.parquet (exists; size check matched)\n",
      "[SKIP] yellow_tripdata_2023-06.parquet (exists; size check matched)\n",
      "[SKIP] yellow_tripdata_2023-07.parquet (exists; size check matched)\n",
      "[SKIP] yellow_tripdata_2023-08.parquet (exists; size check matched)\n",
      "[SKIP] yellow_tripdata_2023-09.parquet (exists; size check matched)\n",
      "[SKIP] yellow_tripdata_2023-10.parquet (exists; size check matched)\n",
      "[SKIP] yellow_tripdata_2023-11.parquet (exists; size check matched)\n",
      "[SKIP] yellow_tripdata_2023-12.parquet (exists; size check matched)\n",
      "[SKIP] fhv_tripdata_2023-01.parquet (exists; size check matched)\n",
      "[SKIP] fhv_tripdata_2023-02.parquet (exists; size check matched)\n",
      "[SKIP] fhv_tripdata_2023-03.parquet (exists; size check matched)\n",
      "[SKIP] fhv_tripdata_2023-04.parquet (exists; size check matched)\n",
      "[SKIP] fhv_tripdata_2023-05.parquet (exists; size check matched)\n",
      "[SKIP] fhv_tripdata_2023-06.parquet (exists; size check matched)\n",
      "[SKIP] fhv_tripdata_2023-07.parquet (exists; size check matched)\n",
      "[SKIP] fhv_tripdata_2023-08.parquet (exists; size check matched)\n",
      "[SKIP] fhv_tripdata_2023-09.parquet (exists; size check matched)\n",
      "[SKIP] fhv_tripdata_2023-10.parquet (exists; size check matched)\n",
      "[SKIP] fhv_tripdata_2023-11.parquet (exists; size check matched)\n",
      "[SKIP] fhv_tripdata_2023-12.parquet (exists; size check matched)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# MAIN LOOP \n",
    "# =========================\n",
    "\n",
    "dt_records = []\n",
    "\n",
    "for yr in years_st:\n",
    "    for dtset in datasts:\n",
    "        out_pt_directy = out_put_rt / str(yr) / dtset\n",
    "        out_pt_directy.mkdir(parents=True, exist_ok=True)\n",
    "        for m in months:\n",
    "            # 1) If local exists, compare local size to remote size (HEAD)\n",
    "            existing = find_local_file(out_pt_directy, dtset, yr, m, file_formt)\n",
    "            if skip_flag and existing:\n",
    "                local_path, local_ext = existing\n",
    "                url_same = build_url(dtset, yr, m, local_ext)\n",
    "                remote_size = Remote_File_content_length(url_same)\n",
    "                local_size  = local_path.stat().st_size\n",
    "                if (remote_size is not None and remote_size == local_size) or (remote_size is None and check_remote_size):\n",
    "                    print(f\"[SKIP] {local_path.name} (exists; size check {'matched' if remote_size else 'not available, use local'})\")\n",
    "                    dt_records.append(OrderedDict([\n",
    "                        ('dataset', dtset),\n",
    "                        ('year', yr),\n",
    "                        ('month', m),\n",
    "                        ('format', local_ext),\n",
    "                        ('filename', str(local_path)),\n",
    "                        ('size_bytes', local_size),\n",
    "                    ]))\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"[RE-DOWNLOAD] {local_path.name} (size mismatch: local={local_size}, remote={remote_size})\")\n",
    "\n",
    "            # download file with from remote server with format\n",
    "            url = build_url(dtset, yr, m, file_formt)\n",
    "            dest = out_pt_directy / f\"{dtset}_tripdata_{yr}-{m:02d}.{file_formt}\"\n",
    "            print('\\nDownloading:', url)\n",
    "            size_bytes = data_download(url, dest)\n",
    "\n",
    "            if size_bytes is None:\n",
    "                print(f\"[SKIP-NO-REMOTE] Missing {yr}-{m:02d} for {dtset} in {FILE_FORMAT}.\")\n",
    "                continue\n",
    "\n",
    "            dt_records.append(OrderedDict([\n",
    "                ('dataset', dtset),\n",
    "                ('year', yr),\n",
    "                ('month', m),\n",
    "                ('format', final_format),\n",
    "                ('filename', str(final_path)),\n",
    "                ('size_bytes', size_bytes),\n",
    "            ]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4525847-3f00-45c0-857f-374331b23799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Record Entries: 36\n",
      "Total size : 6.18 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>format</th>\n",
       "      <th>size_GB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fhvhv</td>\n",
       "      <td>parquet</td>\n",
       "      <td>5.421024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yellow</td>\n",
       "      <td>parquet</td>\n",
       "      <td>0.592082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fhv</td>\n",
       "      <td>parquet</td>\n",
       "      <td>0.168593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset   format   size_GB\n",
       "1   fhvhv  parquet  5.421024\n",
       "2  yellow  parquet  0.592082\n",
       "0     fhv  parquet  0.168593"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Write file sizes to CSV and cumpute total size file TLC data\n",
    "df = pd.DataFrame(dt_records)\n",
    "manifest_csv  = out_put_rt / f\"manifest_{min(years_st)}_{max(years_st)}_size_only.csv\"\n",
    "if not df.empty:\n",
    "    df.to_csv(manifest_csv, index=False)\n",
    "    total_size_gb = df['size_bytes'].sum() / (1024**3)\n",
    "    print(f'\\nData Record Entries: {len(df)}')\n",
    "    print(f'Total size : {total_size_gb:.2f} GB')\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        by_ds = (df.groupby(['dataset','format'])['size_bytes'].sum().reset_index())\n",
    "        by_ds['size_GB'] = by_ds['size_bytes'] / (1024**3)\n",
    "        display(by_ds[['dataset','format','size_GB']].sort_values('size_GB', ascending=False))\n",
    "    except Exception:\n",
    "        pass\n",
    "else:\n",
    "    print(' empty set ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb9ea244-a7d9-45f4-9e54-0693f7785a03",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Point' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# NYC point (lat, lon). We'll request records in NYC local time to match TLC timestamps.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m nyc \u001b[38;5;241m=\u001b[39m Point(\u001b[38;5;241m40.7128\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m74.0060\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching hourly weather from Meteostat… (Jan 2023 → Dec 2024)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m wx \u001b[38;5;241m=\u001b[39m Hourly(nyc, START, END, timezone\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAmerica/New_York\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfetch()  \u001b[38;5;66;03m# hourly DF indexed by time\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Point' is not defined"
     ]
    }
   ],
   "source": [
    "# NYC point (lat, lon). We'll request records in NYC local time to match TLC timestamps.\n",
    "nyc = Point(40.7128, -74.0060)\n",
    "\n",
    "print(\"Fetching hourly weather from Meteostat… (Jan 2023 → Dec 2024)\")\n",
    "wx = Hourly(nyc, START, END, timezone=\"America/New_York\").fetch()  # hourly DF indexed by time\n",
    "\n",
    "# Partition & write monthly Parquet for reproducibility / manageable file sizes\n",
    "wx = wx.copy()\n",
    "wx[\"year\"]  = wx.index.year\n",
    "wx[\"month\"] = wx.index.month\n",
    "\n",
    "written = 0\n",
    "total_rows = 0\n",
    "for (y, m), part in wx.groupby([\"year\", \"month\"], sort=True):\n",
    "    dest = OUT_WEATHER / f\"weather_hourly_{y}-{m:02d}.parquet\"\n",
    "    if dest.exists():\n",
    "        print(f\"[SKIP] {dest.name} already exists\")\n",
    "        continue\n",
    "    part.drop(columns=[\"year\", \"month\"]).to_parquet(dest, index=True)\n",
    "    written += 1\n",
    "    total_rows += len(part)\n",
    "\n",
    "print(f\"Done. Wrote {written} files, {total_rows:,} rows total under {OUT_WEATHER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c543bc-ad77-449d-ad57-8b3c2dbfafea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_if_needed(url: str, dest: Path):\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    remote = head_content_length(url)\n",
    "    if dest.exists():\n",
    "        local = dest.stat().st_size\n",
    "        if (remote and remote == local) or (remote is None):\n",
    "            print(f\"[SKIP] {dest.name} (exists, size check {'matched' if remote else 'n/a'})\")\n",
    "            return\n",
    "        print(f\"[RE-DOWNLOAD] {dest.name} (local {local} vs remote {remote})\")\n",
    "    print(\"Downloading:\", url)\n",
    "    with requests.get(url, stream=True, timeout=60) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(dest, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024*1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    print(f\"[OK] {dest.name} → {dest.stat().st_size:,} bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f159c177-5bb1-4b25-b3c1-47e9644939a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEO_DIR = Path(\"data/geo\")\n",
    "GEO_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL_LOOKUP = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\"\n",
    "URL_ZONES  = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zones.zip\"  # Shapefile zip\n",
    "\n",
    "CSV_LOOKUP = GEO_DIR / \"taxi_zone_lookup.csv\"\n",
    "ZIP_ZONES  = GEO_DIR / \"taxi_zones.zip\"\n",
    "SHAPE_DIR  = GEO_DIR / \"taxi_zones_shp\"\n",
    "\n",
    "download_if_needed(URL_LOOKUP, CSV_LOOKUP)\n",
    "download_if_needed(URL_ZONES,  ZIP_ZONES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c21bb91-ea46-4004-848f-b78a4daf1eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract shapefile if not already\n",
    "if not (SHAPE_DIR.exists() and any(p.suffix.lower()==\".shp\" for p in SHAPE_DIR.rglob(\"*\"))):\n",
    "    print(\"Extracting shapefile…\")\n",
    "    SHAPE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    with zipfile.ZipFile(ZIP_ZONES, \"r\") as z:\n",
    "        z.extractall(SHAPE_DIR)\n",
    "    print(\"Extracted to:\", SHAPE_DIR.resolve())\n",
    "else:\n",
    "    print(\"[SKIP] Shapefile already extracted at\", SHAPE_DIR.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765f74ea-315b-452f-ae39-2051c117593d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0777918d-f982-4a9b-997c-5f999c4f55be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert shapefile → GeoJSON with names joined from the lookup CSV\n",
    "\n",
    "\n",
    "# Find the .shp path\n",
    "shp_candidates = list(SHAPE_DIR.rglob(\"*.shp\"))\n",
    "assert shp_candidates, f\"No .shp found under {SHAPE_DIR}\"\n",
    "shp_path = shp_candidates[0]\n",
    "\n",
    "zones_gdf = gpd.read_file(shp_path)        # should include 'LocationID'\n",
    "lookup_df = pd.read_csv(CSV_LOOKUP)        # has 'LocationID','Borough','Zone','service_zone'\n",
    "\n",
    "zones_named = zones_gdf.merge(lookup_df, on=\"LocationID\", how=\"left\")\n",
    "out_geojson = GEO_DIR / \"taxi_zones.geojson\"\n",
    "zones_named.to_file(out_geojson, driver=\"GeoJSON\")\n",
    "print(\"Wrote\", out_geojson.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6286544-1646-4959-ab78-5b2d30be16fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
