{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feb1a7d8-f89d-4586-8128-0dd42535ca7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas is installed\n",
      "pyarrow is installed\n",
      "requests is installed\n",
      "tqdm is installed\n",
      "matplotlib is installed\n",
      "seaborn is installed\n",
      "numpy is installed\n",
      "boto3 is installed\n",
      "botocore is installed\n",
      "geopandas is installed\n",
      "shapely is installed\n",
      "fiona is installed\n",
      "pyproj is installed\n",
      "rtree is installed\n",
      "folium is installed\n",
      "meteostat is installed\n"
     ]
    }
   ],
   "source": [
    "# Package installation and imports - run this first\n",
    "import importlib.util\n",
    "packages = [ 'pandas', 'pyarrow', 'requests', 'tqdm', 'matplotlib', \n",
    "            'seaborn', 'numpy', 'boto3', 'botocore', 'geopandas', 'shapely', \n",
    "            'fiona', 'pyproj', 'rtree','folium', 'meteostat']\n",
    "for package_name in packages:\n",
    "    is_present = importlib.util.find_spec(package_name)\n",
    "    if is_present is None:\n",
    "        print(f\"{package_name} is not installed\")\n",
    "        !pip install {package_name}\n",
    "        print(f\"{package_name} is now installed\")\n",
    "    else:\n",
    "        print(f\"{package_name} is installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2953a731-1775-451b-a237-54bf46f27f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import time, os\n",
    "from meteostat import Point, Hourly\n",
    "from typing import Optional, Tuple\n",
    "import requests\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    _HAS_TQDM = True\n",
    "except Exception:\n",
    "    _HAS_TQDM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a78d3366-f2a1-4b2a-87bf-c57f30007fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output root: C:\\Users\\kolobet01\\MIT805-Semester-Project-Assignment\\data\\tlc\n"
     ]
    }
   ],
   "source": [
    "years_st = [2023,2024]\n",
    "url_base = 'https://d37ci6vzurychx.cloudfront.net/trip-data'\n",
    "datasts = ['fhvhv', 'yellow', 'fhv']   # taxi data sets to be downloaded\n",
    "months = list(range(1, 13))             # all months of the year\n",
    "file_formt = 'parquet'                 # TLC shares files as parquet\n",
    "out_put_rt = Path('data/tlc')          # folder to download data to\n",
    "\n",
    "start = datetime(min(years_st), 1, 1)      # start time for weather download\n",
    "end   = datetime(max(years_st), 12, 31, 23, 59) \n",
    "\n",
    "weather_folder = Path(\"data/weather/hourly\")  # Folder to download weather data to\n",
    "weather_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "skip_flag = True # flag to avoid multiple downloads of the same file\n",
    "check_remote_size = True  # if no Content-Length from remote server , use local file\n",
    "\n",
    "out_put_rt.mkdir(parents=True, exist_ok=True)\n",
    "print('Output root:', out_put_rt.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e727182-3ff0-43d7-bbc9-396be0d8c1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddc3373-5712-49e4-bf03-24b32bc3b390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfbaf421-8191-4b06-85ee-211c6b3bf606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define url that combines the data link, year, months and extention to get correct data fro TLC\n",
    "def build_url(dataset: str, year: int, month: int, ext: str) -> str:\n",
    "    ds = dataset.lower()\n",
    "    return f\"{url_base}/{ds}_tripdata_{year}-{month:02d}.{ext}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60dbc31f-158f-4a8d-b889-f8def18bb6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return remote Content-Length to compare with local lenth if file has previously been downloaded\n",
    "def Remote_File_content_length(url: str, timeout: float = 30.0):\n",
    "\n",
    "    try:\n",
    "        r = requests.head(url, timeout=timeout, allow_redirects=True)\n",
    "        if r.status_code == 200:\n",
    "            cl = r.headers.get(\"Content-Length\")\n",
    "            return int(cl) if cl is not None else None\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89ba2e2c-d215-43ff-b8e8-e6f7c8683da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to download file from rmote server\n",
    "# download file, show progress, retry and save to folder\n",
    "def data_download(url: str, dest: Path, max_retries: int = 3, backoff: float = 2.0):\n",
    "    headers = {'User-Agent': 'nyc-tlc-downloader/size-only/1.0'}\n",
    "    for attempt in range(1, max_retries+1):\n",
    "        try:\n",
    "            with requests.get(url, stream=True, timeout=60, headers=headers) as r:\n",
    "                if r.status_code == 404:\n",
    "                    print('file not found:', url)\n",
    "                    return None\n",
    "                r.raise_for_status()\n",
    "                total = int(r.headers.get('Content-Length', 0))\n",
    "                dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "                pbar = tqdm(total=total, unit='B', unit_scale=True, desc=dest.name) if (_HAS_TQDM and total) else None\n",
    "                with dest.open('wb') as f:\n",
    "                    for chunk in r.iter_content(chunk_size=1024*1024):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                            if pbar: pbar.update(len(chunk))\n",
    "                if pbar: pbar.close()\n",
    "                return dest.stat().st_size\n",
    "        except Exception as e:\n",
    "            wait = backoff ** attempt\n",
    "            print(f'Error downloading {url} (attempt {attempt}/{max_retries}): {e}. Retrying in {wait:.1f}s...')\n",
    "            time.sleep(wait)\n",
    "    print(f'Failed to download after {max_retries} attempts:', url)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5170b496-e4d3-48fe-8a06-e962b18dcc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search through local file before downloading and return if found\n",
    "\n",
    "def find_local_file(out_dir: Path, dataset: str, year: int, month: int, prefer_ext: str = 'parquet'):\n",
    "    cand = out_dir / f\"{dataset}_tripdata_{year}-{month:02d}.parquet\"\n",
    "    return (cand, 'parquet') if cand.exists() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2128b023-5c9f-451e-a848-bf807cab2505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[skip] fhvhv_tripdata_2023-01.parquet (local file exists matched)\n",
      "[skip] fhvhv_tripdata_2023-02.parquet (local file exists matched)\n",
      "[skip] fhvhv_tripdata_2023-03.parquet (local file exists matched)\n",
      "[skip] fhvhv_tripdata_2023-04.parquet (local file exists matched)\n",
      "[skip] fhvhv_tripdata_2023-05.parquet (local file exists matched)\n",
      "[skip] fhvhv_tripdata_2023-06.parquet (local file exists matched)\n",
      "[skip] fhvhv_tripdata_2023-07.parquet (local file exists matched)\n",
      "[skip] fhvhv_tripdata_2023-08.parquet (local file exists matched)\n",
      "[skip] fhvhv_tripdata_2023-09.parquet (local file exists matched)\n",
      "[skip] fhvhv_tripdata_2023-10.parquet (local file exists matched)\n",
      "[skip] fhvhv_tripdata_2023-11.parquet (local file exists matched)\n",
      "[skip] fhvhv_tripdata_2023-12.parquet (local file exists matched)\n",
      "[skip] yellow_tripdata_2023-01.parquet (local file exists matched)\n",
      "[skip] yellow_tripdata_2023-02.parquet (local file exists matched)\n",
      "[skip] yellow_tripdata_2023-03.parquet (local file exists matched)\n",
      "[skip] yellow_tripdata_2023-04.parquet (local file exists matched)\n",
      "[skip] yellow_tripdata_2023-05.parquet (local file exists matched)\n",
      "[skip] yellow_tripdata_2023-06.parquet (local file exists matched)\n",
      "[skip] yellow_tripdata_2023-07.parquet (local file exists matched)\n",
      "[skip] yellow_tripdata_2023-08.parquet (local file exists matched)\n",
      "[skip] yellow_tripdata_2023-09.parquet (local file exists matched)\n",
      "[skip] yellow_tripdata_2023-10.parquet (local file exists matched)\n",
      "[skip] yellow_tripdata_2023-11.parquet (local file exists matched)\n",
      "[skip] yellow_tripdata_2023-12.parquet (local file exists matched)\n",
      "[skip] fhv_tripdata_2023-01.parquet (local file exists matched)\n",
      "[skip] fhv_tripdata_2023-02.parquet (local file exists matched)\n",
      "[skip] fhv_tripdata_2023-03.parquet (local file exists matched)\n",
      "[skip] fhv_tripdata_2023-04.parquet (local file exists matched)\n",
      "[skip] fhv_tripdata_2023-05.parquet (local file exists matched)\n",
      "[skip] fhv_tripdata_2023-06.parquet (local file exists matched)\n",
      "[skip] fhv_tripdata_2023-07.parquet (local file exists matched)\n",
      "[skip] fhv_tripdata_2023-08.parquet (local file exists matched)\n",
      "[skip] fhv_tripdata_2023-09.parquet (local file exists matched)\n",
      "[skip] fhv_tripdata_2023-10.parquet (local file exists matched)\n",
      "[skip] fhv_tripdata_2023-11.parquet (local file exists matched)\n",
      "[skip] fhv_tripdata_2023-12.parquet (local file exists matched)\n",
      "[skip] fhvhv_tripdata_2024-01.parquet (local file exists matched)\n",
      "[skip] fhvhv_tripdata_2024-02.parquet (local file exists matched)\n",
      "[skip] fhvhv_tripdata_2024-03.parquet (local file exists matched)\n",
      "[skip] fhvhv_tripdata_2024-04.parquet (local file exists matched)\n",
      "[skip] fhvhv_tripdata_2024-05.parquet (local file exists matched)\n",
      "[skip] fhvhv_tripdata_2024-06.parquet (local file exists matched)\n",
      "[skip] fhvhv_tripdata_2024-07.parquet (local file exists matched)\n",
      "[skip] fhvhv_tripdata_2024-08.parquet (local file exists matched)\n",
      "[skip] fhvhv_tripdata_2024-09.parquet (local file exists matched)\n",
      "[skip] fhvhv_tripdata_2024-10.parquet (local file exists matched)\n",
      "[skip] fhvhv_tripdata_2024-11.parquet (local file exists matched)\n",
      "[skip] fhvhv_tripdata_2024-12.parquet (local file exists matched)\n",
      "[skip] yellow_tripdata_2024-01.parquet (local file exists matched)\n",
      "[skip] yellow_tripdata_2024-02.parquet (local file exists matched)\n",
      "[skip] yellow_tripdata_2024-03.parquet (local file exists matched)\n",
      "[skip] yellow_tripdata_2024-04.parquet (local file exists matched)\n",
      "[skip] yellow_tripdata_2024-05.parquet (local file exists matched)\n",
      "[skip] yellow_tripdata_2024-06.parquet (local file exists matched)\n",
      "[skip] yellow_tripdata_2024-07.parquet (local file exists matched)\n",
      "[skip] yellow_tripdata_2024-08.parquet (local file exists matched)\n",
      "[skip] yellow_tripdata_2024-09.parquet (local file exists matched)\n",
      "[skip] yellow_tripdata_2024-10.parquet (local file exists matched)\n",
      "[skip] yellow_tripdata_2024-11.parquet (local file exists matched)\n",
      "[skip] yellow_tripdata_2024-12.parquet (local file exists matched)\n",
      "[skip] fhv_tripdata_2024-01.parquet (local file exists matched)\n",
      "[skip] fhv_tripdata_2024-02.parquet (local file exists matched)\n",
      "[skip] fhv_tripdata_2024-03.parquet (local file exists matched)\n",
      "[skip] fhv_tripdata_2024-04.parquet (local file exists matched)\n",
      "[skip] fhv_tripdata_2024-05.parquet (local file exists matched)\n",
      "[skip] fhv_tripdata_2024-06.parquet (local file exists matched)\n",
      "[skip] fhv_tripdata_2024-07.parquet (local file exists matched)\n",
      "[skip] fhv_tripdata_2024-08.parquet (local file exists matched)\n",
      "[skip] fhv_tripdata_2024-09.parquet (local file exists matched)\n",
      "[skip] fhv_tripdata_2024-10.parquet (local file exists matched)\n",
      "[skip] fhv_tripdata_2024-11.parquet (local file exists matched)\n",
      "[skip] fhv_tripdata_2024-12.parquet (local file exists matched)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# MAIN LOOP \n",
    "# =========================\n",
    "\n",
    "dt_records = []\n",
    "\n",
    "for yr in years_st:\n",
    "    for dtset in datasts:\n",
    "        out_pt_directy = out_put_rt / str(yr) / dtset\n",
    "        out_pt_directy.mkdir(parents=True, exist_ok=True)\n",
    "        for m in months:\n",
    "            # If local exists, compare local size to remote size (HEAD)\n",
    "            existing = find_local_file(out_pt_directy, dtset, yr, m, file_formt)\n",
    "            if skip_flag and existing:\n",
    "                local_path, local_ext = existing\n",
    "                url_same = build_url(dtset, yr, m, local_ext)\n",
    "                \n",
    "                remote_size = Remote_File_content_length(url_same)\n",
    "                local_size  = local_path.stat().st_size\n",
    "                if (remote_size is not None and remote_size == local_size) or (remote_size is None and check_remote_size):\n",
    "                    print(f\"[skip] {local_path.name} (local file exists {'matched' if remote_size else 'not available, use local'})\")\n",
    "                    dt_records.append(OrderedDict([\n",
    "                        ('dataset', dtset),\n",
    "                        ('year', yr),\n",
    "                        ('month', m),\n",
    "                        ('format', local_ext),\n",
    "                        ('filename', str(local_path)),\n",
    "                        ('size_bytes', local_size),\n",
    "                    ]))\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"[RE-DOWNLOAD] {local_path.name} (size mismatch: local={local_size}, remote={remote_size})\")\n",
    "\n",
    "            # download file from remote server with format\n",
    "            url = build_url(dtset, yr, m, file_formt)\n",
    "            dest = out_pt_directy / f\"{dtset}_tripdata_{yr}-{m:02d}.{file_formt}\"\n",
    "            print('\\nDownloading:', url)\n",
    "            size_bytes = data_download(url, dest)\n",
    "\n",
    "            if size_bytes is None:\n",
    "                print(f\"[Skip No Remote] Missing {yr}-{m:02d} for {dtset} in {file_formt}.\")\n",
    "                continue\n",
    "\n",
    "            dt_records.append(OrderedDict([\n",
    "                ('dataset', dtset),\n",
    "                ('year', yr),\n",
    "                ('month', m),\n",
    "                ('format', file_formt),\n",
    "                ('filename', str(dest)),\n",
    "                ('size_bytes', size_bytes),\n",
    "            ]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9ba9cd8-a79a-49ab-8353-56a9b9082a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Record Entries: 72\n",
      "Total size : 12.39 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>format</th>\n",
       "      <th>size_GB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fhvhv</td>\n",
       "      <td>parquet</td>\n",
       "      <td>10.800386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yellow</td>\n",
       "      <td>parquet</td>\n",
       "      <td>1.237491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fhv</td>\n",
       "      <td>parquet</td>\n",
       "      <td>0.356010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset   format    size_GB\n",
       "1   fhvhv  parquet  10.800386\n",
       "2  yellow  parquet   1.237491\n",
       "0     fhv  parquet   0.356010"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Write file sizes to CSV and cumpute total size file TLC data\n",
    "df = pd.DataFrame(dt_records)\n",
    "manifest_csv  = out_put_rt / f\"manifest_{min(years_st)}_{max(years_st)}_size_only.csv\"\n",
    "if not df.empty:\n",
    "    df.to_csv(manifest_csv, index=False)\n",
    "    total_size_gb = df['size_bytes'].sum() / (1024**3)\n",
    "    print(f'\\nData Record Entries: {len(df)}')\n",
    "    print(f'Total size : {total_size_gb:.2f} GB')\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        by_ds = (df.groupby(['dataset','format'])['size_bytes'].sum().reset_index())\n",
    "        by_ds['size_GB'] = by_ds['size_bytes'] / (1024**3)\n",
    "        display(by_ds[['dataset','format','size_GB']].sort_values('size_GB', ascending=False))\n",
    "    except Exception:\n",
    "        pass\n",
    "else:\n",
    "    print(' empty set ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b7939a3-8d90-427e-a1da-d2e63ca8bea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching hourly weather from Meteostat… (Jan 2023 → Dec 2024)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'START' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m nyc \u001b[38;5;241m=\u001b[39m Point(\u001b[38;5;241m40.7128\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m74.0060\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching hourly weather from Meteostat… (Jan 2023 → Dec 2024)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m wx \u001b[38;5;241m=\u001b[39m Hourly(nyc, START, END, timezone\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAmerica/New_York\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfetch()  \u001b[38;5;66;03m# hourly DF indexed by time\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Partition & write monthly Parquet for reproducibility / manageable file sizes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m wx \u001b[38;5;241m=\u001b[39m wx\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'START' is not defined"
     ]
    }
   ],
   "source": [
    "# NYC point (lat, lon). We'll request records in NYC local time to match TLC timestamps.\n",
    "nyc = Point(40.7128, -74.0060)\n",
    "\n",
    "print(\"Fetching hourly weather from Meteostat… (Jan 2023 → Dec 2024)\")\n",
    "wx = Hourly(nyc, START, END, timezone=\"America/New_York\").fetch()  # hourly DF indexed by time\n",
    "\n",
    "# Partition & write monthly Parquet for reproducibility / manageable file sizes\n",
    "wx = wx.copy()\n",
    "wx[\"year\"]  = wx.index.year\n",
    "wx[\"month\"] = wx.index.month\n",
    "\n",
    "written = 0\n",
    "total_rows = 0\n",
    "for (y, m), part in wx.groupby([\"year\", \"month\"], sort=True):\n",
    "    dest = OUT_WEATHER / f\"weather_hourly_{y}-{m:02d}.parquet\"\n",
    "    if dest.exists():\n",
    "        print(f\"[SKIP] {dest.name} already exists\")\n",
    "        continue\n",
    "    part.drop(columns=[\"year\", \"month\"]).to_parquet(dest, index=True)\n",
    "    written += 1\n",
    "    total_rows += len(part)\n",
    "\n",
    "print(f\"Done. Wrote {written} files, {total_rows:,} rows total under {OUT_WEATHER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c543bc-ad77-449d-ad57-8b3c2dbfafea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_loopup_file(url: str, dest: Path):\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    remote = Remote_File_content_length(url)\n",
    "    if dest.exists():\n",
    "        local = dest.stat().st_size\n",
    "        if (remote and remote == local) or (remote is None):\n",
    "            print(f\"[Skip] {dest.name} (exists, size check {'matched' if remote else 'n/a'})\")\n",
    "            return\n",
    "        print(f\"[RE-DOWNLOAD] {dest.name} (local {local} vs remote {remote})\")\n",
    "    print(\"Downloading:\", url)\n",
    "    with requests.get(url, stream=True, timeout=60) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(dest, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024*1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    print(f\"[OK] {dest.name} → {dest.stat().st_size:,} bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f159c177-5bb1-4b25-b3c1-47e9644939a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_data_dir = Path(\"data/geo\")\n",
    "geo_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "url_look_table = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\"\n",
    "url_zone_shape  = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zones.zip\"  # Shapefile zip\n",
    "\n",
    "zone_lookup_csv = geo_data_dir / \"taxi_zone_lookup.csv\"\n",
    "zones_zip  = geo_data_dir / \"taxi_zones.zip\"\n",
    "zone_shape  = geo_data_dir / \"taxi_zones_shp\"\n",
    "\n",
    "download_loopup_file(url_look_table, zone_lookup_csv)\n",
    "download_loopup_file(url_zone_shape,  zones_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765f74ea-315b-452f-ae39-2051c117593d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6286544-1646-4959-ab78-5b2d30be16fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect monthly parquet files\n",
    "w_files = sorted(weather_folder.glob(\"*.parquet\"))\n",
    "\n",
    "if not w_files:\n",
    "    raise SystemExit(f\"No weather parquet files found in: {WEATHER_DIR.resolve()}\")\n",
    "\n",
    "# Build single-line summary\n",
    "size_bytes = sum(p.stat().st_size for p in w_files)\n",
    "df = pd.DataFrame([{\n",
    "    \"dataset\": \"weather_hourly\",\n",
    "    \"format\": \"parquet\",\n",
    "    \"files\": len(w_files),\n",
    "    \"size_GB\": size_bytes / (1024**3),\n",
    "}])\n",
    "\n",
    "\n",
    "print(f\"Hourly Weather Entries: {len(w_files)}\")\n",
    "print(f\"Total size : {size_bytes / (1024**3):.2f} GB\")\n",
    "\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(df[[\"dataset\", \"format\", \"files\", \"size_GB\"]])\n",
    "except Exception:\n",
    "    print(df[[\"dataset\", \"format\", \"files\", \"size_GB\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068afb37-005f-4b29-948e-5f9f80a84050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
