{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4087c367-68ad-434f-861d-6fef7a13c65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#****************MIT805 Assigment part 1  *************************\n",
    "#****************Name: Tankiso Kolobe     *************************\n",
    "#****************Studen no. 25723007     **************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad4c6507-fb3f-49bc-90a1-e46a71b060ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas is installed\n",
      "pyarrow is installed\n",
      "requests is installed\n",
      "tqdm is installed\n",
      "geopandas is installed\n",
      "shapely is installed\n",
      "IPython is installed\n",
      "fiona is installed\n",
      "pyproj is installed\n",
      "folium is installed\n",
      "meteostat is installed\n",
      "pathlib is installed\n",
      "datetime is installed\n",
      "collections is installed\n"
     ]
    }
   ],
   "source": [
    "# Package installation and imports - run this first\n",
    "import importlib.util, sys, subprocess\n",
    "packages = [ 'pandas', 'pyarrow', 'requests', 'tqdm', \n",
    "             'geopandas', 'shapely', 'IPython',\n",
    "            'fiona', 'pyproj', 'folium', 'meteostat',\n",
    "           'pathlib', 'datetime', 'collections']\n",
    "for package_name in packages:\n",
    "    is_present = importlib.util.find_spec(package_name)\n",
    "    if is_present is None:\n",
    "        print(f\"{package_name} is not installed\")\n",
    "        !pip install {package_name}\n",
    "        print(f\"{package_name} is now installed\")\n",
    "    else:\n",
    "        print(f\"{package_name} is installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2953a731-1775-451b-a237-54bf46f27f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import time, os\n",
    "from meteostat import Point, Hourly\n",
    "from typing import Optional, Tuple\n",
    "import requests\n",
    "from IPython.display import display\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    _HAS_TQDM = True\n",
    "except Exception:\n",
    "    _HAS_TQDM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a78d3366-f2a1-4b2a-87bf-c57f30007fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output root: C:\\Users\\kolobet01\\MIT805-Semester-Project-Assignment\\data\\tlc\n"
     ]
    }
   ],
   "source": [
    "years_st = [2023,2024]\n",
    "url_base = 'https://d37ci6vzurychx.cloudfront.net/trip-data'\n",
    "datasts = ['fhvhv', 'yellow', 'fhv']   # taxi data sets to be downloaded\n",
    "months = list(range(1, 13))             # all months of the year\n",
    "file_formt = 'parquet'                 # TLC shares files as parquet\n",
    "out_put_rt = Path('data/tlc')          # folder to download data to\n",
    "\n",
    "start = datetime(min(years_st), 1, 1)      # start time for weather download\n",
    "end   = datetime(max(years_st), 12, 31, 23, 59) \n",
    "\n",
    "weather_folder = Path(\"data/weather/hourly\")  # Folder to download weather data to\n",
    "weather_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "skip_flag = True # flag to avoid multiple downloads of the same file\n",
    "check_remote_size = True  # if no Content-Length from remote server , use local file\n",
    "\n",
    "out_put_rt.mkdir(parents=True, exist_ok=True)\n",
    "print('Output root:', out_put_rt.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb1a7d8-f89d-4586-8128-0dd42535ca7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfbaf421-8191-4b06-85ee-211c6b3bf606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define url that combines the data link, year, months and extention to get correct data fro TLC\n",
    "def build_url(dataset: str, year: int, month: int, ext: str) -> str:\n",
    "    ds = dataset.lower()\n",
    "    return f\"{url_base}/{ds}_tripdata_{year}-{month:02d}.{ext}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60dbc31f-158f-4a8d-b889-f8def18bb6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return remote Content-Length to compare with local lenth if file has previously been downloaded\n",
    "def Remote_File_content_length(url: str, timeout: float = 30.0):\n",
    "\n",
    "    try:\n",
    "        #request hearder from url and allow redirection\n",
    "        rqst = requests.head(url, timeout=timeout, allow_redirects=True)\n",
    "        if rqst.status_code == 200:\n",
    "            cl = rqst.headers.get(\"Content-Length\") #get actual content lenght header\n",
    "            return int(cl) if cl is not None else None #convert to int if present; return None if header missing\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89ba2e2c-d215-43ff-b8e8-e6f7c8683da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to download file from rmote server\n",
    "# download file, show progress, retry and save to folder\n",
    "\n",
    "\n",
    "def data_download(url: str, dest: Path, max_retries: int = 3, backoff: float = 2.0):\n",
    "    #make sure folder exists \n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    attempt = 0\n",
    "    while attempt < max_retries: # retries download in cases of network cuts\n",
    "        try:\n",
    "            # fetch url, stream to avoind overloding and limit hangeing downloads\n",
    "            with requests.get(url, stream=True, timeout=60) as r:\n",
    "                r.raise_for_status() # for http error \n",
    "                #open folder to write to\n",
    "                with dest.open(\"wb\") as file:\n",
    "                    \n",
    "                    for chunk in r.iter_content(chunk_size=1024*1024): #download in chunks\n",
    "                        if chunk:\n",
    "                            file.write(chunk) # write file\n",
    "            #return bytes size                   \n",
    "            return dest.stat().st_size  # success\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            if attempt >= max_retries:\n",
    "                print(f\"[Download failed] {url} → {e.__class__.__name__}: {e}\")\n",
    "                return None\n",
    "            time.sleep(backoff ** attempt) #Exponential backoff duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5170b496-e4d3-48fe-8a06-e962b18dcc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search through local file before downloading and return if found\n",
    "\n",
    "def find_local_file(out_dir: Path, dataset: str, year: int, month: int, prefer_ext: str = 'parquet'):\n",
    "    cand = out_dir / f\"{dataset}_tripdata_{year}-{month:02d}.parquet\"\n",
    "    return (cand, 'parquet') if cand.exists() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2128b023-5c9f-451e-a848-bf807cab2505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN LOOP \n",
    "dt_records = []\n",
    "\n",
    "for yr in years_st: # iteration over the years\n",
    "    for dtset in datasts: # iteration over datesets or taxi types\n",
    "        out_pt_directy = out_put_rt / str(yr) / dtset # output folder per and per taxi type\n",
    "        out_pt_directy.mkdir(parents=True, exist_ok=True)\n",
    "        for m in months: # iterations in mothns per per dataset\n",
    "            \n",
    "            # If local exists, compare local size to remote size (HEAD)\n",
    "            existing_file = find_local_file(out_pt_directy, dtset, yr, m, file_formt)\n",
    "            if skip_flag and existing_file:\n",
    "                local_path, local_ext = existing_file\n",
    "                url_same = build_url(dtset, yr, m, local_ext) # url for header check\n",
    "                \n",
    "                remote_size = Remote_File_content_length(url_same)\n",
    "                local_size  = local_path.stat().st_size # local size for comparison\n",
    "\n",
    "                #check remote size, compare with local\n",
    "                if (remote_size is not None and remote_size == local_size) or (remote_size is None and check_remote_size):\n",
    "                    #update data records\n",
    "                    dt_records.append(OrderedDict([\n",
    "                        ('dataset', dtset),\n",
    "                        ('year', yr),\n",
    "                        ('month', m),\n",
    "                        ('format', local_ext),\n",
    "                        ('filename', str(local_path)),\n",
    "                        ('size_bytes', local_size),\n",
    "                    ]))\n",
    "                    continue\n",
    "                else:\n",
    "                    #re-attemt a mismatched file\n",
    "                    print(f\"[Re-download] {local_path.name} (size mismatch: local={local_size}, remote={remote_size})\")\n",
    "\n",
    "            # build url for requsted file\n",
    "            url = build_url(dtset, yr, m, file_formt)\n",
    "            dest = out_pt_directy / f\"{dtset}_tripdata_{yr}-{m:02d}.{file_formt}\" # path for downloaded data\n",
    "            print('\\nDownloading:', url)\n",
    "            size_bytes = data_download(url, dest) #data download \n",
    "\n",
    "            if size_bytes is None:\n",
    "                print(f\"[Skip No Remote] Missing {yr}-{m:02d} for {dtset} in {file_formt}.\")\n",
    "                continue\n",
    "            #add successful download to data records\n",
    "            dt_records.append(OrderedDict([\n",
    "                ('dataset', dtset),\n",
    "                ('year', yr),\n",
    "                ('month', m),\n",
    "                ('format', file_formt),\n",
    "                ('filename', str(dest)),\n",
    "                ('size_bytes', size_bytes),\n",
    "            ]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9ba9cd8-a79a-49ab-8353-56a9b9082a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Record Entries: 72\n",
      "Total size : 12.39 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>format</th>\n",
       "      <th>size_GB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fhvhv</td>\n",
       "      <td>parquet</td>\n",
       "      <td>10.800386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yellow</td>\n",
       "      <td>parquet</td>\n",
       "      <td>1.237491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fhv</td>\n",
       "      <td>parquet</td>\n",
       "      <td>0.356010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset   format    size_GB\n",
       "1   fhvhv  parquet  10.800386\n",
       "2  yellow  parquet   1.237491\n",
       "0     fhv  parquet   0.356010"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Write file sizes to CSV and cumpute total size file TLC data\n",
    "df = pd.DataFrame(dt_records)\n",
    "#folder for the data per year\n",
    "record_csv  = out_put_rt / f\"data_records_{min(years_st)}_{max(years_st)}_size.csv\"\n",
    "if not df.empty:\n",
    "    df.to_csv(record_csv, index=False) # write data records to csv\n",
    "    total_size_GB = df['size_bytes'].sum() / (1024**3) #total size in GB\n",
    "    print(f'\\nData Record Entries: {len(df)}')\n",
    "    print(f'Total size : {total_size_GB:.2f} GB')\n",
    "    # group be taxityepe or datas set\n",
    "    by_ds = (df.groupby(['dataset','format'])['size_bytes'].sum().reset_index())\n",
    "    by_ds['size_GB'] = by_ds['size_bytes'] / (1024**3)\n",
    "    display(by_ds[['dataset','format','size_GB']].sort_values('size_GB', ascending=False))\n",
    "else:\n",
    "    print(' empty set ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce2b0326-a62a-43de-a6e7-f0e3eba7e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b7939a3-8d90-427e-a1da-d2e63ca8bea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching hourly weather from Meteostat… (Jan 2023 → Dec 2024)\n",
      "[Skip] weather_hourly_2023-01.parquet already exists\n",
      "[Skip] weather_hourly_2023-02.parquet already exists\n",
      "[Skip] weather_hourly_2023-03.parquet already exists\n",
      "[Skip] weather_hourly_2023-04.parquet already exists\n",
      "[Skip] weather_hourly_2023-05.parquet already exists\n",
      "[Skip] weather_hourly_2023-06.parquet already exists\n",
      "[Skip] weather_hourly_2023-07.parquet already exists\n",
      "[Skip] weather_hourly_2023-08.parquet already exists\n",
      "[Skip] weather_hourly_2023-09.parquet already exists\n",
      "[Skip] weather_hourly_2023-10.parquet already exists\n",
      "[Skip] weather_hourly_2023-11.parquet already exists\n",
      "[Skip] weather_hourly_2023-12.parquet already exists\n",
      "[Skip] weather_hourly_2024-01.parquet already exists\n",
      "[Skip] weather_hourly_2024-02.parquet already exists\n",
      "[Skip] weather_hourly_2024-03.parquet already exists\n",
      "[Skip] weather_hourly_2024-04.parquet already exists\n",
      "[Skip] weather_hourly_2024-05.parquet already exists\n",
      "[Skip] weather_hourly_2024-06.parquet already exists\n",
      "[Skip] weather_hourly_2024-07.parquet already exists\n",
      "[Skip] weather_hourly_2024-08.parquet already exists\n",
      "[Skip] weather_hourly_2024-09.parquet already exists\n",
      "[Skip] weather_hourly_2024-10.parquet already exists\n",
      "[Skip] weather_hourly_2024-11.parquet already exists\n",
      "[Skip] weather_hourly_2024-12.parquet already exists\n",
      "Done. 0 files saved, 0 rows total under data\\weather\\hourly\n"
     ]
    }
   ],
   "source": [
    "# dowload houly weather data for NYC\n",
    "\n",
    "# NewYork City point (lat, lon). \n",
    "nyc = Point(40.7128, -74.0060)\n",
    "\n",
    "print(\"Fetching hourly weather from Meteostat… (Jan 2023 → Dec 2024)\")\n",
    "weather_hourly = Hourly(nyc, start, end, timezone=\"America/New_York\").fetch()  # fetch hourly weather for NYC \n",
    "\n",
    "# Partitioned parquet files\n",
    "weather_hourly = weather_hourly.copy()\n",
    "\n",
    "weather_hourly[\"year\"]  = weather_hourly.index.year\n",
    "weather_hourly[\"month\"] = weather_hourly.index.month\n",
    "\n",
    "saved_files = 0\n",
    "total_rows = 0\n",
    "#group by year and month\n",
    "for (y, m), part in weather_hourly.groupby([\"year\", \"month\"], sort=True):\n",
    "    dest = weather_folder / f\"weather_hourly_{y}-{m:02d}.parquet\" # parquet file \n",
    "    if dest.exists(): # avoid redownload\n",
    "        print(f\"[Skip] {dest.name} already exists\")\n",
    "        continue\n",
    "    # wremove helper columns before writing \n",
    "    part.drop(columns=[\"year\", \"month\"]).to_parquet(dest, index=True)\n",
    "    saved_files += 1\n",
    "    total_rows += len(part)\n",
    "\n",
    "print(f\"Done. {saved_files} files saved, {total_rows:,} rows total under {weather_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78c543bc-ad77-449d-ad57-8b3c2dbfafea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#down load and safe zone look up table csv file\n",
    "\n",
    "def download_loopup_file(url: str, dest: Path):\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    remote = Remote_File_content_length(url)\n",
    "    if dest.exists():\n",
    "        local = dest.stat().st_size # loca file size\n",
    "        if (remote and remote == local) or (remote is None): # skip if local file already exist\n",
    "            \n",
    "            return\n",
    "        print(f\"[Re-download] {dest.name} (local {local} vs remote {remote})\")\n",
    "    print(\"Downloading:\", url)\n",
    "    #perform a http request \n",
    "    with requests.get(url, stream=True, timeout=60) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(dest, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024*1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    print(f\"[OK] {dest.name} → {dest.stat().st_size:,} bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f159c177-5bb1-4b25-b3c1-47e9644939a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_data_dir = Path(\"data/geo\")\n",
    "geo_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "url_look_table = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\"\n",
    "url_zone_shape  = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zones.zip\"  # Shapefile zip\n",
    "\n",
    "zone_lookup_csv = geo_data_dir / \"taxi_zone_lookup.csv\"\n",
    "zones_zip  = geo_data_dir / \"taxi_zones.zip\"\n",
    "zone_shape  = geo_data_dir / \"taxi_zones_shp\"\n",
    "\n",
    "download_loopup_file(url_look_table, zone_lookup_csv)\n",
    "download_loopup_file(url_zone_shape,  zones_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765f74ea-315b-452f-ae39-2051c117593d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6286544-1646-4959-ab78-5b2d30be16fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hourly Weather Entries: 24\n",
      "Total size : 0.0005 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>format</th>\n",
       "      <th>files</th>\n",
       "      <th>size_GB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weather_hourly</td>\n",
       "      <td>parquet</td>\n",
       "      <td>24</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          dataset   format  files   size_GB\n",
       "0  weather_hourly  parquet     24  0.000478"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print total weather file size and count after download\n",
    "\n",
    "weather_files = sorted(weather_folder.glob(\"*.parquet\"))\n",
    "\n",
    "if not weather_files:\n",
    "    print(f\"No weather files found in: {weather_folder.resolve()}\")\n",
    "\n",
    "# Build single-line summary\n",
    "size_bytes = sum(file.stat().st_size for file in weather_files)\n",
    "df = pd.DataFrame([{\n",
    "    \"dataset\": \"weather_hourly\",\n",
    "    \"format\": \"parquet\",\n",
    "    \"files\": len(weather_files),\n",
    "    \"size_GB\": size_bytes / (1024**3),\n",
    "}])\n",
    "\n",
    "\n",
    "print(f\"Hourly Weather Entries: {len(weather_files)}\")\n",
    "print(f\"Total size : {size_bytes / (1024**3):.4f} GB\")\n",
    "\n",
    "display(df[[\"dataset\", \"format\", \"files\", \"size_GB\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068afb37-005f-4b29-948e-5f9f80a84050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
